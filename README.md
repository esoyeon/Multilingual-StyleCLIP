# Multilingual-StyleCLIP

Global direction inference colab
Latent optimization inference colab
Latent mapper inference colab

## Overview
 Since the release of CLIP by OpenAPI, multiple applications of this multi-modal model have been made, including StyleCLIP. StyleCLIP is a combination of high-resolution image generator - StyleGAN and text-image connecter - CLIP. By measuring cosine similarities of text vector generated by CLIP and image vector generated by StyleGAN, StyleCLIP makes it possible to conveniently manipulate an image with a text prompt. 

 We further extended the benefits of StyleCLIP by implementing Multilingual-CLIP to this model. Multilingual-CLIP consists of two encoders: an image encoder and a fine-tuned text encoder that is capable of encoding any language. Thus, our version of StyleCLIP manipulates an image not only with an English text prompt, but also with a text prompt in any other language, for example in Korean. 
 
 Accuracy of image encoding task also has increased. Official image encoder in StyleCLIP is Encoder4Encoding(e4e) which plays its role when training and testing. However empirically we found out that the result of e4e is quite different from the original input image. To overcome this issue, we encoded data sets for training a mapper and images for inference with Restyle Encoder. Restlye Encoder which was introduced in the paper “ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement (ICCV 2021)” iteratively self-corrects the inverted latent code, resulting in increased accuracy. 

This repository contains:
-	Pytorch training code for Multilingual Latent Optimizatizer, Latent Mapper, Global Direction
-	Pytorch inference code Multilingual Latent Optimizatizer, Latent Mapper, Global Direction
-	latent mapper, global direction weights
-	CelebA-HQ Dataset latents (encoded via Restlye)
-	Restlye encoder applied over pSp pretrained on the FFHQ dataset
-	Huggingface available transformer M-BERT Base ViT-B (???)
-	CLIP
-	StyleGAN2

## Setup
The experiment was done in following conditions:
- Python 3.7.12
-	Torch 1.10.0+cu11
-	Google Colab

# Usage
## Latent optimization
The code relies on Rosinality pytorch implementation of StyleGAN2. Facial Recognition weights and pretrained restyle encoder are to be downloaded here.
- --description is for the driving text (can be in any language).
-	To control the manipulation effect, adjust l2 lambda and ID lambda parameters

## Latent mapper
The code relies on Rosinality pytorch implementation of StyleGAN2. Facial Recognition weights and pretrained restyle encoder are to be downloaded here.
### training
-	To resume a training, provide --checkpoint_path.
-	--description is for the driving text (can be in any language).
-	To control the manipulation effect, adjust l2 lambda and ID lambda parameters
-	Takes up 10 hours for proper training

### Inference
-	For inference, we provide several pretrained mappers (text prompt in Korean language)

## Global Direction 
The code relies on the official TensorFlow implementation of StyleGAN2. Facial Recognition weights and pretrained restyle encoder are to be downloaded here.

# Editing Examples
Images below are of real people, and were inverted into latent space via Restyle Encoder.

### Latent optimization

### Latent mapper
![getting-started](image\\image1.png)

### Global Direction 

